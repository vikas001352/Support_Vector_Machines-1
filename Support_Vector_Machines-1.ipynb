{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd6f57-5669-40cc-bfcd-a9a4230cd41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "\n",
    "ANS-1\n",
    "\n",
    "\n",
    "A linear Support Vector Machine (SVM) is a binary classification algorithm that tries to find the best hyperplane that separates two classes of data points in a high-dimensional space. In the case of linearly separable data, the hyperplane effectively acts as a decision boundary.\n",
    "\n",
    "The mathematical formula for a linear SVM can be represented as follows:\n",
    "\n",
    "Given a dataset with labeled examples:\n",
    "\n",
    "Training data: {(x1, y1), (x2, y2), ..., (xn, yn)}\n",
    "\n",
    "where xi represents the feature vector of the ith data point, and yi represents its corresponding class label (either -1 or +1).\n",
    "\n",
    "The goal of the linear SVM is to find the weight vector w and bias term b such that the decision function f(xi) = sign(w · xi + b) correctly classifies the data points, where \"·\" denotes the dot product.\n",
    "\n",
    "The decision function f(xi) takes the sign of the dot product of the weight vector and the feature vector plus the bias term. If the result is positive, the data point is classified as the +1 class; if it's negative, it's classified as the -1 class.\n",
    "\n",
    "The linear SVM aims to maximize the margin between the two classes while ensuring that the data points are correctly classified. The margin is the perpendicular distance between the hyperplane and the nearest data points of each class.\n",
    "\n",
    "The optimization problem for the linear SVM can be formulated as:\n",
    "\n",
    "minimize: ||w||^2 / 2\n",
    "\n",
    "subject to: yi * (w · xi + b) ≥ 1 for all i=1, 2, ..., n\n",
    "\n",
    "Where ||w|| represents the L2 norm of the weight vector w, and the objective is to minimize this norm to maximize the margin. The inequality constraint ensures that the data points are correctly classified by the decision function.\n",
    "\n",
    "Solving this optimization problem results in finding the optimal weight vector w and bias term b that define the hyperplane and provide the best separation of the two classes. This hyperplane is the decision boundary of the linear SVM.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "\n",
    "ANS-2\n",
    "\n",
    "\n",
    "The objective function of a linear Support Vector Machine (SVM) is to find the parameters (weight vector and bias term) that define the hyperplane separating two classes of data points while maximizing the margin between the classes. The margin is the perpendicular distance between the hyperplane and the nearest data points of each class.\n",
    "\n",
    "The objective function for a linear SVM is formulated as follows:\n",
    "\n",
    "minimize: ||w||^2 / 2\n",
    "\n",
    "subject to: yi * (w · xi + b) ≥ 1 for all i=1, 2, ..., n\n",
    "\n",
    "Where:\n",
    "- w represents the weight vector that defines the orientation of the hyperplane.\n",
    "- b is the bias term that shifts the hyperplane away from the origin.\n",
    "- xi is the feature vector of the ith data point.\n",
    "- yi is the class label of the ith data point (+1 for the positive class and -1 for the negative class).\n",
    "- ||w|| represents the L2 norm (Euclidean norm) of the weight vector w.\n",
    "\n",
    "The objective of the linear SVM is to minimize the L2 norm of the weight vector ||w||^2 / 2 while satisfying the inequality constraint yi * (w · xi + b) ≥ 1 for all data points. This constraint ensures that the data points are correctly classified by the decision function and are on the correct side of the margin.\n",
    "\n",
    "Geometrically, minimizing ||w||^2 / 2 corresponds to maximizing the margin between the two classes. By minimizing the squared L2 norm of the weight vector, the SVM effectively searches for a hyperplane that separates the data points with the widest margin, leading to better generalization and improved performance on unseen data.\n",
    "\n",
    "The optimization problem is a convex quadratic programming problem, and various optimization techniques, such as the Sequential Minimal Optimization (SMO) algorithm, can be used to find the optimal weight vector w and bias term b that define the decision boundary of the linear SVM.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "\n",
    "\n",
    "ANS-3\n",
    "\n",
    "\n",
    "The kernel trick is a powerful concept in Support Vector Machines (SVM) that allows the SVM to efficiently work in high-dimensional feature spaces without explicitly computing the transformations to those spaces. It enables the SVM to handle non-linearly separable data by implicitly transforming the original feature space into a higher-dimensional space where the data points become linearly separable. This transformation is done by using a kernel function.\n",
    "\n",
    "In the context of SVM, a kernel is a function that calculates the dot product of the transformed feature vectors in the higher-dimensional space without explicitly computing the transformation. Instead of directly applying the SVM in the original feature space, the kernel trick allows us to operate in the higher-dimensional space in an implicit manner.\n",
    "\n",
    "The general idea of the kernel trick can be mathematically represented as follows:\n",
    "\n",
    "Suppose we have a dataset with n data points represented by their feature vectors: {(x1, y1), (x2, y2), ..., (xn, yn)}, where xi represents the feature vector of the ith data point, and yi represents its corresponding class label (+1 or -1).\n",
    "\n",
    "The decision function in the high-dimensional space is represented as:\n",
    "\n",
    "f(x) = sign(Σ(ai * yi * K(xi, x) + b)\n",
    "\n",
    "Where:\n",
    "- ai are the Lagrange multipliers obtained during the SVM optimization process.\n",
    "- b is the bias term.\n",
    "- K(xi, x) is the kernel function that computes the dot product between the transformed feature vectors in the higher-dimensional space.\n",
    "\n",
    "The most commonly used kernel functions are:\n",
    "1. Linear Kernel: K(xi, x) = xi · x (dot product in the original feature space).\n",
    "2. Polynomial Kernel: K(xi, x) = (γ * (xi · x) + r)^d, where γ and r are user-defined parameters, and d is the degree of the polynomial.\n",
    "3. Radial Basis Function (RBF) Kernel (Gaussian Kernel): K(xi, x) = exp(-γ * ||xi - x||^2), where γ is a user-defined parameter.\n",
    "\n",
    "The key advantage of the kernel trick is that it avoids the explicit computation of the high-dimensional feature space, which can be computationally expensive or even infeasible for very high dimensions. By using kernels, SVM can efficiently handle non-linearly separable data and achieve a flexible decision boundary that captures complex patterns in the data. This makes the kernel trick a fundamental concept that significantly enhances the versatility and applicability of SVM in various machine learning tasks.\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    " Q4. What is the role of support vectors in SVM Explain with example\n",
    "            \n",
    "            \n",
    "  ANS-4\n",
    "            \n",
    "            \n",
    "     In Support Vector Machines (SVM), support vectors play a crucial role in defining the decision boundary and determining the optimal hyperplane that separates the classes of data points. Support vectors are the data points that lie closest to the decision boundary, and they are the ones that have the most influence on the final decision boundary.\n",
    "\n",
    "The key idea behind SVM is to find the hyperplane that maximizes the margin between the two classes while minimizing the classification error. The margin is the perpendicular distance between the hyperplane and the closest data points of each class, which are the support vectors.\n",
    "\n",
    "Let's explain the role of support vectors with a simple example:\n",
    "\n",
    "Imagine we have a 2D dataset with two classes: blue circles and red squares. The goal is to find the best decision boundary (line) that separates the two classes.\n",
    "\n",
    "```\n",
    "Blue Circles: (2, 3), (3, 4), (3, 6)\n",
    "Red Squares: (6, 5), (7, 3), (8, 4)\n",
    "```\n",
    "\n",
    "In the 2D space, the decision boundary is a line, and we want to find the line that maximizes the margin between the blue circles and red squares. The support vectors are the data points that are closest to the decision boundary, which are the ones lying on the margin or misclassified points.\n",
    "\n",
    "Suppose we found a decision boundary (line) that separates the classes as follows:\n",
    "\n",
    "```\n",
    "Decision Boundary (Line): y = 0.8x + 1\n",
    "```\n",
    "\n",
    "The decision boundary is the line `y = 0.8x + 1`. The margin is the distance between the two parallel lines that are equidistant from the decision boundary and do not contain any data points. The support vectors are the data points closest to the decision boundary and lie exactly on these two parallel lines.\n",
    "\n",
    "In this example, the support vectors are:\n",
    "```\n",
    "Support Vectors: (2, 3), (6, 5), (8, 4)\n",
    "```\n",
    "\n",
    "These three points are crucial for defining the decision boundary and the margin. They have the most influence on the optimal hyperplane since they are closest to it. The other data points further from the decision boundary do not significantly impact the decision boundary.\n",
    "\n",
    "Support vectors are essential because they help make SVM a sparse model. In many datasets, the majority of data points do not affect the decision boundary. By focusing on the support vectors, SVM can efficiently separate the classes and generalize well to new data.\n",
    "\n",
    "In summary, support vectors are the critical data points that determine the decision boundary in SVM. They are the data points closest to the decision boundary and have the most influence on defining the optimal hyperplane. By focusing on these support vectors, SVM achieves a sparse representation and makes the algorithm computationally efficient and effective in handling non-linearly separable data.\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    " Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "            \n",
    "            \n",
    "    ANS-5\n",
    "            \n",
    "            \n",
    "            \n",
    "       Sure! Let's illustrate the concepts of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM with examples and graphs.\n",
    "\n",
    "**Example Data:**\n",
    "Suppose we have a 2D dataset with two classes, represented by blue circles and red squares:\n",
    "\n",
    "```\n",
    "Blue Circles: (2, 3), (3, 4), (4, 5), (4, 6)\n",
    "Red Squares: (1, 2), (5, 3), (6, 4), (7, 5)\n",
    "```\n",
    "\n",
    "Let's visualize the data points on a scatter plot:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data points\n",
    "blue_circles = [(2, 3), (3, 4), (4, 5), (4, 6)]\n",
    "red_squares = [(1, 2), (5, 3), (6, 4), (7, 5)]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(*zip(*blue_circles), color='blue', marker='o', label='Blue Circles')\n",
    "plt.scatter(*zip(*red_squares), color='red', marker='s', label='Red Squares')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Hyperplane:**\n",
    "The hyperplane is the decision boundary that separates the two classes in the feature space. In a 2D space, the hyperplane is a line. In higher dimensions, it becomes a hyperplane. In a linear SVM, the hyperplane is the best line that maximizes the margin between the two classes.\n",
    "\n",
    "**Graph of Hyperplane:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Define a linear SVM hyperplane (line) equation: y = mx + c\n",
    "m = 0.5\n",
    "c = 0\n",
    "\n",
    "# Generate x values for the line\n",
    "x_values = np.linspace(0, 8, 100)\n",
    "\n",
    "# Calculate corresponding y values using the equation of the line\n",
    "y_values = m * x_values + c\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(*zip(*blue_circles), color='blue', marker='o', label='Blue Circles')\n",
    "plt.scatter(*zip(*red_squares), color='red', marker='s', label='Red Squares')\n",
    "\n",
    "# Plot the hyperplane\n",
    "plt.plot(x_values, y_values, 'k-', label='Hyperplane (Decision Boundary)')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Marginal Plane:**\n",
    "The marginal plane is defined by the two parallel lines that are equidistant from the hyperplane and do not contain any data points. It is used to define the margin in the SVM.\n",
    "\n",
    "**Graph of Marginal Plane:**\n",
    "\n",
    "```python\n",
    "# Define the parallel lines (marginal planes) that are equidistant from the hyperplane\n",
    "margin_distance = 1\n",
    "upper_margin_line = y_values + margin_distance\n",
    "lower_margin_line = y_values - margin_distance\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(*zip(*blue_circles), color='blue', marker='o', label='Blue Circles')\n",
    "plt.scatter(*zip(*red_squares), color='red', marker='s', label='Red Squares')\n",
    "\n",
    "# Plot the hyperplane\n",
    "plt.plot(x_values, y_values, 'k-', label='Hyperplane (Decision Boundary)')\n",
    "\n",
    "# Plot the marginal planes\n",
    "plt.plot(x_values, upper_margin_line, 'r--', label='Upper Marginal Plane')\n",
    "plt.plot(x_values, lower_margin_line, 'r--', label='Lower Marginal Plane')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Hard Margin vs. Soft Margin:**\n",
    "In a Hard Margin SVM, the goal is to find a hyperplane that perfectly separates the two classes without any misclassifications. This is only possible if the data is linearly separable. However, if the data is not linearly separable, a Hard Margin SVM will fail to find a valid hyperplane.\n",
    "\n",
    "In contrast, a Soft Margin SVM allows for some misclassifications to handle non-linearly separable data. It introduces a penalty for misclassifications and aims to find a hyperplane that maximizes the margin while minimizing the number of misclassifications.\n",
    "\n",
    "**Graph of Hard Margin and Soft Margin:**\n",
    "\n",
    "```python\n",
    "# Define the hyperplane (line) equation for a hard margin\n",
    "hard_margin_m = 1\n",
    "hard_margin_c = -2\n",
    "\n",
    "# Define the hyperplane (line) equation for a soft margin\n",
    "soft_margin_m = 0.7\n",
    "soft_margin_c = -1\n",
    "\n",
    "# Calculate corresponding y values using the equation of the hard margin line\n",
    "hard_margin_y_values = hard_margin_m * x_values + hard_margin_c\n",
    "\n",
    "# Calculate corresponding y values using the equation of the soft margin line\n",
    "soft_margin_y_values = soft_margin_m * x_values + soft_margin_c\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(*zip(*blue_circles), color='blue', marker='o', label='Blue Circles')\n",
    "plt.scatter(*zip(*red_squares), color='red', marker='s', label='Red Squares')\n",
    "\n",
    "# Plot the hard margin hyperplane\n",
    "plt.plot(x_values, hard_margin_y_values, 'k-', label='Hard Margin Hyperplane')\n",
    "\n",
    "# Plot the soft margin hyperplane\n",
    "plt.plot(x_values, soft_margin_y_values, 'g--', label='Soft Margin Hyperplane')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "In the graph, the green dashed line represents the Soft Margin Hyperplane, which allows some misclassifications, while the black solid line represents the Hard Margin Hyperplane, which does not allow any misclassifications.\n",
    "\n",
    "I hope these examples and graphs help illustrate the concepts of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM.\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "Q6. SVM Implementation through Iris dataset.\n",
    "            \n",
    "            \n",
    "            \n",
    "ANS-6\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
